\begin{algorithm}[H]\fontfamily{lmss}
\caption{ID3 Textbook Algorithm \fixme{Change the textbook algorithm, make similar to the id3.py}}\label{a:id3-simple}
\begin{algorithmic}[1]
\Procedure{ID3Textbook}{$examples, classAttribute, attributes$}
    \If {$\textsc{AllExamplesSame}(examples)$}
        \State \textbf{return} $examples[0][classAttribute]$\Comment{\small{Return a node with the label that is same \par\hfill to all examples}}
    \ElsIf {$\textsc{Length}(attributes) = 0$}\Comment{\small{If number of predicting attributes is empty}}
        \State \textbf{return} $\textsc{MostCommonLabel}(examples)$\Comment{\small{Return the label with the most common \par\hfill value of the target attribute in the examples}}
    \EndIf

    \State $bestAttribute \gets \textsc{Best}(examples, attributes)$\Comment{\small{Best attribute is the one with maximum \par\hfill information gain -- or similarly, minimum entropy}}
    
    \State $branches \gets \{\}$
    \For{each possible value $v_i$ of $bestAttribute$}
        \State Add a new tree branch below $root$, corresponding to the test $bestAttribute \gets v_i$

        \State $subset \gets$ all examples that $example[\textsc{Index}(bestAttribute)] = v_i$
        
        \If {$\textsc{Length}(subset) = 0$}
            \State $branch \gets \textsc{MostCommonLabel}(examples))$\Comment{\small{Add a leaf}}
        \Else
            \State $branch \gets \textsc{ID3}(subset,$ $attributes - \{bestAttribute\})$\Comment{\small{Recurse and add the subtree}}
        \EndIf
        \State $branches \gets branches \cup branch$
    \EndFor
    \State \textbf{return} $branches$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]\fontfamily{lmss}
\caption{Entropy Textbook Algorithm}\label{a:id3-Entropy-simple}
\begin{algorithmic}[1]
\Procedure{Entropy}{$examples$}\Comment{\small{Entropy (H(S) equation \ref{eq:entropy}), is a measure of the amount of \par\hfill uncertainty in the dataset}}
    \State $entropy \gets 0$
    
    \For{each possible value $v_i$ of $classAttribute$}
        \State $count \gets$ number of examples that $examples[classAttribute] = v_i$
        
        \State $percentage \gets count / \textsc{Length}(examples)$
        \If {$percentage \neq 0$}
            \State $entropy \gets entropy - (percentage * log_2 (percentage))$
        \EndIf
    \EndFor
    \State \textbf{return} $entropy$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]\fontfamily{lmss}
\caption{Information Gain Textbook Algorithm}\label{a:id3-gain-simple}
\begin{algorithmic}[1]
\Procedure{InformationGain}{$examples, attribute$}\Comment{\small{Information gain (equation \ref{eq:gain}) is the \par\hfill measure of the difference in entropy from before to after a dataset is split on an attribute}}
    \State $gain \gets \textsc{Entropy}(examples)$
    
    \For{each possible value $v_i$ of $attribute$}
        \State $subset \gets$ all examples that $examples[\textsc{Index}(attribute)] = v_i$
        
        \State $percentage \gets \textsc{Length}(subset) / \textsc{Length}(examples)$
        \If {$percentage \neq 0$}
            \State $gain \gets gain - (percentage * \textsc{Entropy}(subset))$
        \EndIf
    \EndFor
    \State \textbf{return} $gain$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]\fontfamily{lmss}
\caption{All Examples Same Textbook Algorithm}\label{a:id3-same-simple}
\begin{algorithmic}[1]
\Procedure{AllExamplesSame}{$examples, attribute$}\Comment{\small{Check if all examples are same, \par\hfill in order to return the same label
}}
    \State $same \gets true$
    \For{$i \in \{0, \dots, \textsc{Length}(examples)-2\}$}\Comment{\small{Check all examples, two at a time}}
        \State $ex_1 \gets examples[i]$
        \State $ex_2 \gets examples[i+1]$
        
        \State $same \gets same \textsc{AND} (ex_1[classAttribute] = ex_2[classAttribute])$
    \EndFor
    \State \textbf{return} $same$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]\fontfamily{lmss}
\caption{Best Textbook Algorithm}\label{a:id3-best-simple}
\begin{algorithmic}[1]
\Procedure{Best}{$examples, attribute$}\Comment{\small{Find the best attribute; the one with maximum \par\hfill information gain -- or similarly, minimum entropy}}
    \State $max_gain \gets \textsc{InformationGain}(examples, attributes[0])$
    \State $best \gets attributes[0]$
    
    \For{$i \in \{1, \dots, \textsc{Length}(attributes)-1\}$}
        \State $gain \gets  \textsc{InformationGain}(examples, attributes[i])$
        \If {$gain > max_gain$}
            \State $max_gain \gets gain$
            \State $best \gets attributes[i]$
        \EndIf
    \EndFor
    \State \textbf{return} $best$
\EndProcedure

\end{algorithmic}
\end{algorithm}

