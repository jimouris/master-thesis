\begin{algorithm}[t]\fontfamily{lmss}
\caption{ID3 Textbook Algorithm \fixme{Change the textbook algorithm, make similar to the id3.py}}\label{a:id3-simple}
\begin{algorithmic}[1]
\Procedure{ID3Textbook}{$examples, classAttribute, attributes$}
    \If {$\textsc{AllExamplesSame}(examples)$}
        \State \textbf{return} $examples[0][classAttribute]$\Comment{\small{Return a node with the label that is same \par\hfill to all examples}}
    \ElsIf {$\textsc{Length}(attributes) = 0$}\Comment{\small{If number of predicting attributes is empty}}
        \State \textbf{return} $\textsc{MostCommonLabel}(examples)$\Comment{\small{Return the label with the most common \par\hfill value of the target attribute in the examples}}
    \EndIf

    \State $bestAttribute \gets \textsc{Best}(examples, attributes)$\Comment{\small{Best attribute is the one with maximum \par\hfill information gain -- or similarly, minimum entropy}}
    
    \State $branches \gets \{\}$
    \For{each possible value $v_i$ of $bestAttribute$}
        \State Add a new tree branch below $root$, corresponding to the test $bestAttribute \gets v_i$

        \State $subset \gets$ all examples that $example[\textsc{Index}(bestAttribute)] = v_i$
        
        \If {$\textsc{Length}(subset) = 0$}
            \State $branch \gets \textsc{MostCommonLabel}(examples))$\Comment{\small{Add a leaf}}
        \Else
            \State $branch \gets \textsc{ID3}(subset,$ $attributes - \{bestAttribute\})$\Comment{\small{Recurse and add the subtree}}
        \EndIf
        \State $branches \gets branches \cup branch$
    \EndFor
    \State \textbf{return} $branches$
\EndProcedure

\begin{spacing}{0.5}
\end{spacing}

\Procedure{Entropy}{$examples$}\Comment{\small{Entropy (H(S) equation \ref{eq:entropy}), is a measure of the amount of \par\hfill uncertainty in the dataset}}
    \State $entropy \gets 0$
    
    \For{each possible value $v_i$ of $classAttribute$}
        \State $count \gets$ number of examples that $examples[classAttribute] = v_i$
        
        \State $percentage \gets count / \textsc{Length}(examples)$
        \If {$percentage \neq 0$}
            \State $entropy \gets entropy - (percentage * log_2 (percentage))$
        \EndIf
    \EndFor
    \State \textbf{return} $entropy$
\EndProcedure

\begin{spacing}{0.5}
\end{spacing}

\Procedure{InformationGain}{$examples, attribute$}\Comment{\small{Information gain (equation \ref{eq:gain}) is the \par\hfill measure of the difference in entropy from before to after a dataset is split on an attribute}}
    \State $gain \gets \textsc{Entropy}(examples)$
    
    \For{each possible value $v_i$ of $attribute$}
        \State $subset \gets$ all examples that $examples[\textsc{Index}(attribute)] = v_i$
        
        \State $percentage \gets \textsc{Length}(subset) / \textsc{Length}(examples)$
        \If {$percentage \neq 0$}
            \State $gain \gets gain - (percentage * \textsc{Entropy}(subset))$
        \EndIf
    \EndFor
    \State \textbf{return} $gain$
\EndProcedure

\end{algorithmic}
\end{algorithm}


