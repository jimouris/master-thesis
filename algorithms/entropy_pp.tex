\begin{algorithm}[H]\fontfamily{lmss}
\caption{Privacy Preserving Entropy Algorithm}\label{a:id3-Entropy-pp}
\begin{algorithmic}[1]
\Require ${\color{darkred}{examples}}, {\color{darkred}{example}}, {\color{darkred}{entropy}}, {\color{darkred}{count}}, {\color{darkred}{percentage}}, {\color{darkred}{eq}}$
\renewcommand{\algorithmicrequire}{\textbf{Global Vars:}}
\Require $classAttribute$
\Procedure{Entropy}{${\color{darkred}{examples}}$}\Comment{{\small Entropy (H(S) equation \ref{eq:entropy}), is a measure of the \par\hfill amount of uncertainty in the dataset}}
    \State ${\color{darkred}{entropy}} \gets \enc{0}$

    \For{each possible class $c_i$ of $classAttribute$}
        \State ${\color{darkred}{eq}} \gets {\color{darkred}{examples}}[classAttribute] \heq c_i$ \Comment{{\small ${\texttt{SIMD}; \color{darkred}{eq}}$ vector gets $\enc{1}$ at positions \par\hfill where the equality holds, $\enc{0}$ otherwise}}
        \State ${\color{darkred}{count}} \gets \textsc{Sum}({\color{darkred}{eq}})$
        \State ${\color{darkred}{percentage}} \gets {\color{darkred}{count}} \div \textsc{CountPositives}({\color{darkred}{examples}})$
        \State ${\color{darkred}{entropy}} \gets {\color{darkred}{entropy}} - ({\color{darkred}{percentage}} * log_2 ({\color{darkred}{percentage}}))$\Comment{{\small Here we use a modified $log_2$ \par\hfill function that returns $\enc{0}$ given $\enc{0}$ as input}}
    \EndFor
    \State \textbf{return} ${\color{darkred}{entropy}}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
