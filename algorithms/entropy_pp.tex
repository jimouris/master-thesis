\begin{algorithm}[H]\fontfamily{lmss}
\caption{Privacy Preserving Entropy Algorithm}\label{a:id3-entropy-pp}
\begin{algorithmic}[1]
\Require $\red{examples}, \red{example}, \red{entropy}, \red{count}, \red{percentage}, \red{eq}$
\renewcommand{\algorithmicrequire}{\textbf{Global Vars:}}
\Require $classAttribute$
\Procedure{Entropy}{$\red{examples}$}\Comment{{\small Entropy (H(S) equation \ref{eq:entropy}), is a measure of the \par\hfill amount of uncertainty in the dataset}}
    \State $\red{entropy} \gets \enc{0}$

    \For{{\small each possible class} $c_i$ {\small of} $classAttribute$}
        \State $\red{eq} \gets \red{examples}[classAttribute] \heq c_i$ \Comment{{\small ${\texttt{SIMD}; \red{eq}}$ vector gets $\enc{1}$ at positions \par\hfill where the equality holds, $\enc{0}$ otherwise}}
        \State $\red{count} \gets \textsc{Sum}(\red{eq})$
        \State $\red{percentage} \gets \red{count} \div \textsc{CountPositives}(\red{examples})$
        \State $\red{entropy} \gets \red{entropy} - (\red{percentage} * log_2 (\red{percentage}))$\Comment{{\small Here we use a modified $log_2$ \par\hfill function that returns $\enc{0}$ given $\enc{0}$ as input}}
    \EndFor
    \State \textbf{return} $\red{entropy}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
