\begin{algorithm}[H]\fontfamily{lmss}
\caption{Privacy Preserving Entropy Algorithm}\label{a:id3-Entropy-pp}
\begin{algorithmic}[1]
\Require ${\color{darkred}{examples}}, {\color{darkred}{example}}, {\color{darkred}{entropy}}, {\color{darkred}{count}}, {\color{darkred}{percentage}}, {\color{darkred}{eq}}$
\renewcommand{\algorithmicrequire}{\textbf{Global Vars:}}
\Require $classAttribute$
\Procedure{Entropy}{${\color{darkred}{examples}}$}\Comment{\small{Entropy (H(S) equation \ref{eq:entropy}), is a measure of the \par\hfill amount of uncertainty in the dataset}}
    \State ${\color{darkred}{entropy}} \gets \enc{0}$

    \For{each possible class $c_i$ of $classAttribute$}
        \For{${\color{darkred}{example}} \in {\color{darkred}{examples}}$}
          \State ${\color{darkred}{eq}} \gets {\color{darkred}{examples}}[classAttribute] \heq c_i$
          \State ${\color{darkred}{count}} \gets {\color{darkred}{count}} \hplus {\color{darkred}{eq}}$
        \EndFor
        \State ${\color{darkred}{percentage}} \gets {\color{darkred}{count}} \div \textsc{Length}({\color{darkred}{examples}})$
        \State ${\color{darkred}{entropy}} \gets {\color{darkred}{entropy}} - ({\color{darkred}{percentage}} * log_2 ({\color{darkred}{percentage}}))$\Comment{\small{Here we use a modified $log_2$ \par\hfill function that returns $\enc{0}$ given $\enc{0}$ as input}}
    \EndFor
    \State \textbf{return} ${\color{darkred}{entropy}}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
